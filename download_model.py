from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "medalpaca/medalpaca-7b"  # ‚úÖ Le bon mod√®le

# ‚ö†Ô∏è Remplace 'token="..."' si tu n'as pas d√©j√† fait `huggingface-cli login`
token = "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"  # Ton token Hugging Face ici

print("üì¶ T√©l√©chargement du tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, token=token)  # Forcer le tokenizer lent

print("üß† T√©l√©chargement du mod√®le...")
model = AutoModelForCausalLM.from_pretrained(model_name, token=token)

# Test rapide avec pipeline de g√©n√©ration de texte
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
prompt = "Un patient se plaint de douleurs abdominales et de naus√©es. Que peut-on suspecter ?"
result = pipe(prompt, max_new_tokens=100)
print(result[0]["generated_text"])
